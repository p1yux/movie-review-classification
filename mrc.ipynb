{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import os, re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud, STOPWORDS  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB  # Import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current working directory\n",
    "print('Current working directory: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "train = pd.read_csv('labeledTrainData.tsv', delimiter='\\t')\n",
    "test = pd.read_csv('testData.tsv', delimiter='\\t')\n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sentiment counts\n",
    "print(\"Number of rows for sentiment 1: {}\".format(len(train[train.sentiment == 1])))\n",
    "print(\"Number of rows for sentiment 0: {}\".format(len(train[train.sentiment == 0])))\n",
    "print(train.groupby('sentiment').describe().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for review length\n",
    "train['length'] = train['review'].apply(len)\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of review length\n",
    "train['length'].plot.hist(bins=100)\n",
    "plt.title('Histogram of Review Length')\n",
    "plt.xlabel('Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check specific review lengths\n",
    "print(train[train['length'] == 13708]['review'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def clean_text(raw_text):\n",
    "    # 1. Remove HTML tags\n",
    "    raw_text = BeautifulSoup(raw_text, \"html.parser\").get_text()\n",
    "    \n",
    "    # 2. Remove all non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    \n",
    "    # 3. Convert to lowercase, split into words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    return [w for w in words if w not in stops]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean review text and add new columns\n",
    "train['clean_review'] = train['review'].apply(clean_text)\n",
    "train['length_clean_review'] = train['clean_review'].apply(len)\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word cloud\n",
    "word_cloud = WordCloud(width=1000, height=500, stopwords=STOPWORDS, background_color='blue').generate(' '.join(train['review']))\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag of Words\n",
    "bow_transform = CountVectorizer(analyzer=clean_text)\n",
    "X_train_counts = bow_transform.fit_transform(train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details about the Bag of Words\n",
    "print(f\"Total number of vocab words: {len(bow_transform.vocabulary_)}\")\n",
    "print(bow_transform.get_feature_names_out()[71821])\n",
    "print(bow_transform.get_feature_names_out()[72911])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF matrix\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for TF-IDF with vocabulary\n",
    "texts = [\"This is a sample document about war.\", \"This document is about a book.\", \"Another document discussing war and book.\"]\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(texts)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vocabulary and IDF values\n",
    "vocabulary = count_vectorizer.vocabulary_\n",
    "word_index_war = vocabulary.get('war')\n",
    "word_index_book = vocabulary.get('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word_index_war is not None and word_index_book is not None:\n",
    "    print(tfidf_transformer.idf_[word_index_war])\n",
    "    print(tfidf_transformer.idf_[word_index_book])\n",
    "else:\n",
    "    print(\"One or both words are not in the vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform new reviews using TF-IDF\n",
    "new_reviews = [\"New review about war\", \"Another book review\"]\n",
    "new_reviews_bow = count_vectorizer.transform(new_reviews)\n",
    "new_reviews_tfidf = tfidf_transformer.transform(new_reviews_bow)\n",
    "print(new_reviews_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['review'], train['sentiment'], test_size=0.22, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def pred(predicted, compare):\n",
    "    cm = pd.crosstab(compare, predicted, rownames=['Actual'], colnames=['Predicted'])\n",
    "    TN = cm.iloc[0, 0]\n",
    "    FP = cm.iloc[0, 1]\n",
    "    FN = cm.iloc[1, 0]\n",
    "    TP = cm.iloc[1, 1]\n",
    "    \n",
    "    print(\"CONFUSION MATRIX ------- >> \")\n",
    "    print(cm)\n",
    "    print()\n",
    "    \n",
    "    # Check accuracy of model\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) * 100\n",
    "    print('Accuracy: ', round(accuracy, 2))\n",
    "    print()\n",
    "    \n",
    "    # False Negative Rate\n",
    "    false_negative_rate = (FN * 100) / (FN + TP)\n",
    "    print('False Negative Rate: ', round(false_negative_rate, 2))\n",
    "    print()\n",
    "    \n",
    "    # False Positive Rate\n",
    "    false_positive_rate = (FP * 100) / (FP + TN)\n",
    "    print('False Positive Rate: ', round(false_positive_rate, 2))\n",
    "    print()\n",
    "    \n",
    "    # Print classification report\n",
    "    print('Classification Report: ')\n",
    "    print(classification_report(compare, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training models\n",
    "# Logistic Regression\n",
    "pipeline_logit = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=clean_text)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', LogisticRegression(random_state=101))\n",
    "])\n",
    "pipeline_logit.fit(X_train, y_train)\n",
    "predictions = pipeline_logit.predict(X_train)\n",
    "pred(predictions, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Result\n",
    "predictions = pipeline_logit.predict(X_test)\n",
    "pred(predictions, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Model\n",
    "pipeline_nb = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=clean_text)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB())  # Use MultinomialNB here\n",
    "])\n",
    "pipeline_nb.fit(X_train, y_train)\n",
    "predictions = pipeline_nb.predict(X_train)\n",
    "pred(predictions, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result on Test Case\n",
    "predictions = pipeline_nb.predict(X_test)\n",
    "pred(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "pipeline_rf = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=clean_text)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=500))\n",
    "])\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "predictions = pipeline_rf.predict(X_train)\n",
    "pred(predictions, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Result\n",
    "predictions = pipeline_rf.predict(X_test)\n",
    "pred(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model: Logistic Regression\n",
    "pipeline_logit_final = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=clean_text)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', LogisticRegression(random_state=101))\n",
    "])\n",
    "pipeline_logit_final.fit(train['review'], train['sentiment'])\n",
    "test['sentiment'] = pipeline_logit_final.predict(test['review'])\n",
    "test.head(5)\n",
    "output = test[['id', 'sentiment']]\n",
    "print(output)\n",
    "output.to_csv(\"output.csv\", index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
